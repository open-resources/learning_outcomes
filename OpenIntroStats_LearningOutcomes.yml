Introduction to Data:
    Topic Outcome:
        - Identify variables as numerical and categorical.
        - If the variable is numerical, further classify as continuous or discrete based on whether or not the variable can take on an infinite number of values or only non-negative whole numbers, respectively.
        - If the variable is categorical, determine if it is ordinal based on whether or not the levels have a natural ordering.
        - Define associated variables as variables that show some relationship with one another. Further categorize this relationship as positive or negative association, when possible.
        - Define variables that are not associated as independent.
        - Identify the explanatory variable in a pair of variables as the variable suspected of affecting the other. However, note that labeling variables as explanatory and response does not guarantee that the relationship between the two is actually causal, even if there is an association identified between the two variables.
        - Classify a study as observational or experimental, and determine and explain why whether the study's results can be generalized to the population and whether they suggest correlation or causation between the variables studied.
        - If random sampling has been employed in data collection, the results should be generalizable to the target population.
        - If random assignment has been employed in study design, the results suggest causality.
        - Question confounding variables and sources of bias in a given study.
        - Distinguish between simple random, stratified, cluster, and multistage sampling, and recognize the benefits and drawbacks of choosing one sampling scheme over another.
        - Simple random sampling: Each subject in the population is equally likely to be selected.
        - Stratified sampling: First divide the population into homogenous strata (subjects within each stratum are similar, across strata are different), then randomly sample from within each strata.
        - Cluster sampling: First divide the population into clusters (subjects within each cluster are non-homogenous, but clusters are similar to each other), then randomly sample a few clusters, and then sample all cases within those clusters.
        - Multistage sampling: First divide the population into clusters, then randomly sample a few clusters, and then randomly sample from within each cluster.
        - Identify the four principles of experimental design and recognize their purposes: control any possible confounders, randomize into treatment and control groups, replicate by using a sufficiently large sample or repeating the experiment, and block any variables that might influence the response.
        - Identify if single or double blinding has been used in a study.
        
# Topic Break

Summarizing Data:
    Topic Outcome:
        - Use scatterplots for describing the relationship between two numerical variables making sure to note the direction (positive or negative), form (linear or non-linear) and the strength of the relationship as well as any unusual observations that stand out.
        - When describing the distribution of a numerical variable, mention its shape, center, and spread, as well as any unusual observations.
        - Note that there are three commonly used measures of center and spread:
        - Identify the shape of a distribution as symmetric, right skewed, or left skewed, and unimodal, bimodal, multimodal, or uniform.
        - Use histograms and box plots to visualize the shape, center, and spread of numerical distributions, and intensity maps for visualizing the spatial distribution of the data.
        - Define a robust statistic (e.g. median, IQR) as measures that are not heavily affected by skewness and extreme outliers, and determine when they are more appropriate measured of center and spread compared to other similar statistics.
        - Recognize when transformations (e.g. log) can make the distribution of data more symmetric, and hence easier to model.
        - Reading: Section 2.1 of OpenIntro Statistics
        - Test yourself:
        - Describe what is meant by robust statistics and when they are used.
        - Describe when and why we might want to apply a log transformation to a variable. \\
        - Use frequency tables and bar plots to describe the distribution of one categorical variable.
        - Use contingency tables and segmented bar plots or mosaic plots to assess the relationship between two categorical variables.
        - Use side-by-side box plots for assessing the relationship between a numerical and a categorical variable.
        - Reading: Section 2.2 of OpenIntro Statistics
        - Test yourself:
        - Interpret the plot in Figure 1.40 (page 39) of the textbook.
        - You collect data on 100 classmates, 70 females and 30 males. 10\% of the class are smokers, and smoking is independent of gender. Calculate how many males and females would be expected to be smokers. Sketch a mosaic plot of this scenario. \\
        - Note that an observed difference in sample statistics suggesting dependence between variables may be due to random chance, and that we need to use hypothesis testing to determine if this difference is too large to be attributed to random chance.
        - Set up null and alternative hypotheses for testing for independence between variables, and evaluate data's support for these hypotheses using a simulation technique.
        - Reading: Section 2.3 of OpenIntro Statistics
        - Test yourself: Explain why difference in sample proportions across two groups does not necessarily indicate dependence between the two variables involved?

# Topic Break

Probability:
    Topic Outcome:
        - Define trial, outcome, and sample space.
        - Explain why the long-run relative frequency of repeated independent events settle down to the true probability as the number of trials increases, i.e. why the law of large numbers holds.
        - Distinguish disjoint (also called mutually exclusive) and independent events.
        - Draw Venn diagrams representing events and their probabilities.
        - Define a probability distribution as a list of the possible outcomes with corresponding probabilities that satisfies three rules:
        - Define complementary outcomes as mutually exclusive outcomes of the same random process whose probabilities add up to 1.
        - Distinguish between union of events (A or B) and intersection of events (A and B).
        - Calculate the probability of union of events using the (general) addition rule.
        - Reading: Section 3.1 of OpenIntro Statistics
        - Test yourself:
        - What is the probability of getting a head on the 6th coin flip if in the first 5 flips the coin landed on a head each time?
        - True / False: Being right handed and having blue eyes are mutually exclusive events.
        - P(A) = 0.5, P(B) = 0.6, there are no other possible outcomes in the sample space. What is P(A and B)?
        - Distinguish marginal and conditional probabilities.
        - Calculate the probability of intersection of independent events using the multiplication rule.
        - Construct tree diagrams to calculate conditional probabilities and probabilities of intersection of non-independent events using Bayes' theorem.
        - Reading: Section 3.2 of OpenIntro Statistics
        - Test yourself: 50\% of students in a class are social science majors and the rest are not. 70\% of the social science students and 40\% of the non-social science students are in a relationship. Create a contingency table and a tree diagram summarizing these probabilities. Calculate the percentage of students in this class who are in a relationship.
        - Sampling without replacement from a small population means we no longer have independence between our observations.
        - A random variable is a random process or variable with a numerical outcome. Modeling a process using a random variable allows us to apply a mathematical framework and statistical principles for better understanding and predicting outcomes in the real world.
        - We use measures of center and spread to define distributions of random variables.
        - Expected value and variance of a discrete random variable, $X$, can be calculated as follows:
        - Standard deviation is the square root of variance. We use standard deviation also as a measure of the variability of the random variable. Standard deviation is often easier to interpret since it's in the same units of the random variable.
        - Linear combinations of random variables:
        - Probability density functions represent the distributions of continuous random variables.
        - Reading: Sections 3.3 - 3.5 of OpenIntro Statistics  

# Topic Break

Distributions of random variables:
    Topic Outcome:
        - Define the standardized (Z) score of a data point as the number of standard deviations it is away from the mean: $Z = \frac{x - \mu}{\sigma}$.
        - Use the Z score
        - Depending on the shape of the distribution determine whether the median would have a negative, positive, or 0 Z score.
        - Assess whether or not a distribution is nearly normal using the 68-95-99.7\% rule or graphical methods such as a normal probability plot.
        - Reading: Section 4.1 of OpenIntro Statistics
        - Test yourself: True/False: In a right skewed distribution the Z score of the median is positive.
        - If X is a random variable that takes the value 1 with probability of success $p$ and 0 with probability of success $1-p$, then $X$ is a Bernoulli random variable.
        - The geometric distribution is used to describe how many trials it takes to observe a success.
        - Define the probability of finding the first success in the $n^{th}$ trial as $(1-p)^{n-1}p$.
        - Determine if a random variable is binomial using the four conditions:
        - Calculate the number of possible scenarios for obtaining $k$ successes in $n$ trials using the choose function: ${n \choose k} = \frac{n!}{k!~(n - k)!}$.
        - Calculate probability of a given number of successes in a given number of trials using the binomial distribution: $P(k = K) = \frac{n!}{k!~(n - k)!}~p^k~(1-p)^{(n - k)}$.
        - Calculate the expected number of successes in a given number of binomial trials $(\mu = np)$ and its standard deviation $(\sigma = \sqrt{np(1-p)})$.
        - When number of trials is sufficiently large ($np \ge 10$ and $n(1-p) \ge 10$), use normal approximation to calculate binomial probabilities, and explain why this approach works.
        - Reading: Section 4.2 and 4.3 of OpenIntro Statistics
        - Test yourself:
        - True/False: We can use the binomial distribution to determine the probability that in 10 rolls of a die the first 6 occurs on the 8th roll.
        - True / False: If a family has 3 kids, there are 8 possible combinations of gender order.
        - True/ False: When $n = 100$ and $p = 0.92$ we can use the normal approximation to the binomial to calculate the probability of 90 or more successes.

# Topic Break

Foundations for inference:
    Topic Outcome:
        - Define sample statistic as a point estimate for a population parameter, for example, the sample proportion is used to estimate the population proportion, and note that point estimate and sample statistic are synonymous.
        - Recognize that point estimates (such as the sample proportion) will vary from one sample to another, and define this variability as sampling variation.
        - Calculate the sampling variability of the proportion, the standard error, as $SE = \sqrt{\frac{p(1-p)}{n}}$, where $p$ is the population proportion.
        - Standard error measures the variability in point estimates from different samples of the same size and from the same population, i.e. measures the sampling variability.
        - Recognize that when the sample size increases we would expect the sampling variability to decrease.
        - Notice that sampling distributions of point estimates coming from samples that don't meet the required conditions for the CLT (about sample size and independence) will not be normal.
        - Reading: Section 5.1 of OpenIntro Statistics
        - Test yourself:
        - For each of the following situations, state whether the variable is categorical or numerical, and whether the parameter of interest is a mean or a proportion.
        - In a survey, college students are asked whether they agree with their parents' political ideology.
        - In a survey, college students are asked what percentage of their non-class time they spend studying.
        - Explain what is going on in Figures 5.4 and 5.5 of the book (pages 176 and 177).
        - Define a confidence interval as the plausible range of values for a population parameter.
        - Define the confidence level as the percentage of random samples which yield confidence intervals that capture the true population parameter.
        - Calculate an approximate 95\% confidence interval by adding and subtracting 2 standard errors to the point estimate: $point~estimate \pm 2 \times SE$.
        - Recognize that the Central Limit Theorem (CLT) is about the distribution of point estimates, and that given certain conditions, this distribution will be nearly normal.
        - [(1)] the observations in the sample are independent, and
        - [(2)] there are at least 10 successes and 10 failures,
        - Recall that independence of observations in a sample is provided by random sampling (in the case of observational studies) or random assignment (in the case of experiments).
        - Recognize that the nearly normal distribution of the point estimate (as suggested by the CLT) implies that a more precise confidence interval can be calculated as
        - Define margin of error as the distance required to travel in either direction away from the point estimate when constructing a confidence interval, i.e. $z^{\star} \times SE$.
        - Interpret a confidence interval as ``We are XX\% confident that the true population parameter is in this interval", where XX\% is the desired confidence level.
        - Reading: Section 5.2 of OpenIntro Statistics
        - Test yourself:
        - Explain, in plain English, what is going on in Figure 5.6 of the book (page 182).
        - List the conditions necessary for the CLT to hold. Make sure to list alternative conditions for when we know the population distribution is normal vs. when we don't know what the population distribution is, and the when the sample size is barely over 30 vs. when it's very large.
        - Confirm that $z^{\star}$ for a 98\% confidence level is 2.33. (Include a sketch of the normal curve in your response.)
        - Explain, in plain English, the difference between standard error and margin of error.
        - Explain how the hypothesis testing framework resembles a court trial.
        - Recognize that in hypothesis testing we evaluate two competing claims:
        - Construction of hypotheses:
        - Define a p-value as the conditional probability of obtaining a sample statistic at least as extreme as the one observed given that the null hypothesis is true.
        - Calculate a p-value as the area under the normal curve beyond the observed sample proportion (either in one tail or both, depending on the alternative hypothesis). Note that in doing so you can use a Z score, where
        - Infer that if a confidence interval does not contain the null value the null hypothesis should be rejected in favor of the alternative.
        - Compare the p-value to the significance level to make a decision between the hypotheses:
        - Note that the conclusion of a hypothesis test might be erroneous regardless of the decision we make.
        - Choose a significance level depending on the risks associated with Type 1 and Type 2 errors.
        - Formulate the framework for statistical inference using hypothesis testing and nearly normal point estimates:
        - [(1)] Set up the hypotheses first in plain language and then using appropriate notation.
        - [(2)] Identify the appropriate sample statistic that can be used as a point estimate for the parameter of interest.
        - [(3)] Verify that the conditions for the CLT holds.
        - [(4)] Compute the SE, sketch the sampling distribution, and shade area(s) representing the p-value.
        - [(5)] Using the sketch and the normal model, calculate the p-value and determine if the null hypothesis should be rejected or not, and state your conclusion in context of the data and the research question.
        - If the conditions necessary for the CLT to hold are not met, note this and do not go forward with the analysis. (We will later learn about methods to use in these situations.)
        - Distinguish statistical significance vs. practical significance.
        - Reading: Section 5.3 of OpenIntro Statistics
        - Test yourself:
        - List errors in the following hypotheses: $H_0: \hat{p} > 0.20$ and $H_A: \hat{p} \ge 0.25$
        - What is wrong with the following statement? \\
        - Suppose a researcher is interested in evaluating the following claim ``The proportion of adults in the US is who vote is 40\%", and that she believes this is an underestimate.
        - How should she set up her hypotheses?
        - Explain to her, in plain language, how she should collect data and carry out a hypothesis test.
        - Go back to Section 2.4 and describe the differences and similarities between the hypothesis testing procedure using simulation and using theory. Especially discuss how the calculation of the p-value changes while the definition stays the same.
        - If we want to decrease the margin of error, and hence have a more precise confidence interval, should we increase or decrease the sample size?
        - In a random sample of 1,017 Americans 60\% said they do not trust the mass media when it comes to reporting the news fully, accurately, and fairly. The standard error associated with this estimate is 0.015 (1.5\%). What is the margin of error for a 95\% confidence level? Calculate a 95\% confidence interval and interpret it in context. You may assume that the point estimate is normally distributed (we'll learn how to check this later).

# Topic Break

Inference for categorical data:
    Topic Outcome:
        - Define population proportion $p$ (parameter) and sample proportion $\hat{p}$ (point estimate).
        - Calculate the sampling variability of the proportion, the standard error, as
        - Recognize that the Central Limit Theorem (CLT) is about the distribution of point estimates, and that given certain conditions, this distribution will be nearly normal.
        - Note that if the CLT doesn?t apply and the sample proportion is low (close to 0) the sampling distribution will likely be right skewed, if the sample proportion is high (close to 1) the sampling distribution will likely be left skewed.
        - Remember that confidence intervals are calculated as
        - Note that the standard error calculation for the confidence interval and the hypothesis test are different when dealing with proportions, since in the hypothesis test we need to assume that the null hypothesis is true -- remember: p-value = P(observed or more extreme test statistic $|$ $H_0$ true).
        - Calculate the required minimum sample size for a given margin of error at a given confidence level, and explain why we use $\hat{p} = 0.5$ if there are no previous studies suggesting a more accurate estimate.
        - Reading: Section 6.1 of OpenIntro Statistics
        - Test yourself:
        - Suppose 10\% of Duke students smoke. You collect many random samples of 100 Duke students at a time, and calculate a sample proportion ($\hat{p}$) for each sample, indicating the proportion of students in that sample who smoke. What would you expect the distribution of these $\hat{p}$s to be? Describe its shape, center, and spread.
        - Suppose you want to construct a confidence interval with a margin of error no more than 4\% for the proportion of Duke students who smoke. How would your calculation of the required sample size change if you don't know anything about the smoking habits of Duke students vs. if you have a reliable previous study estimating that about 10\% of Duke students smoke.
        - Note that the calculation of the standard error of the distribution of the difference in two independent sample proportions is different for a confidence interval and a hypothesis test.
        - Note that the reason for the difference in calculations of standard error is the same as in the case of the single proportion: when the null hypothesis claims that the two population proportions are equal, we need to take that into consideration when calculating the standard error for the hypothesis test, and use a common proportion for both samples.
        - Reading: Section 6.2 of OpenIntro Statistics
        - Test yourself:
        - Suppose a 95\% confidence interval for the difference between the Duke and UNC students who smoke (calculated using $\hat{p}_{Duke} - \hat{p}_UNC$) is (-0.08,0.11). Interpret this interval, making sure to incorporate into your interpretation a comparative statement about the two schools.
        - Does the above interval suggest a significant difference between the true proportions of smokers at the two schools?
        - Suppose you had a sample of 100 students from Duke where 11 of them smoke, and a sample of 80 students from UNC where 10 of them smoke. Calculate $\hat{p}_{pool}$.
        - When and why do we use $\hat{p}_{pool}$ in calculation of the standard error for the difference between two sample proportions?
        - Use a chi-square test of goodness of fit to evaluate if the distribution of levels of a single categorical variable follows a hypothesized distribution.
        - [] $H_0:$ The distribution of the variable follows the hypothesized distribution, and any observed differences are due to chance.
        - [] $H_A:$ The distribution of the variable does not follow the hypothesized distribution.
        - Calculate the expected counts for a given level (cell) in a one-way table as the sample size times the hypothesized proportion for that level.
        - Calculate the chi-square test statistic as
        - Note that the chi-square distribution is right skewed with one parameter: degrees of freedom. In the case of a goodness of fit test, $df = \# \text{of categories} - 1$.
        - List the conditions necessary for performing a chi-square test (goodness of fit or independence)
        - [(1)] the observations should be independent
        - [(2)] expected counts for each cell should be at least 5
        - [(3)] degrees of freedom should be at least 2 (if not, use methods for evaluating proportions)
        - Describe how to use the chi-square table to obtain a p-value.
        - Reading: Section 6.3 of OpenIntro Statistics
        - Test yourself:
        - Explain the different hypothesis tests one could use when assessing the distribution of a categorical variable (e.g. smoking status) with only two levels (e.g. levels: smoker and non-smoker) vs. more than two levels (e.g. levels: heavy smoker, moderate smoker, occasional smoker, non-smoker).
        - Why is the p-value for chi-square tests always ``one sided"?
        - When evaluating the independence of two categorical variables where at least one has more than two levels, use a chi-square test of independence.
        - [] $H_0:$ The two variables are independent.
        - [] $H_A:$ The two variables are dependent.
        - Calculate expected counts in two-way tables as
        - Calculate the degrees of freedom for chi-square test of independence as $df = (R - 1) \times (C - 1)$, where $R$ is the number of rows in a two-way table, and $C$ is the number of columns.
        - Note that there is no such thing as a chi-square confidence interval for proportions, since in the case of a categorical variables with many levels, there isn't one parameter to estimate.
        - Reading: Section 6.4 of OpenIntro Statistics
        - Test yourself:
        - What are the null and alternative hypotheses in chi-square test of independence?
        - Suppose a chi-square test of independence between two categorical variables (one with 5, the other with 3 levels) is yields a test statistic of $\chi^2 = 14$. What's the conclusion of the hypothesis test at 5\% significance level?
        - Use simulation methods when sample size conditions aren't met for inference for categorical variables.
        - In hypothesis testing
        - Use bootstrap methods for confidence intervals for categorical variables with at most two levels.
        - Reading: Sections 6.5 and 6.6 of OpenIntro Statistics
        - Test yourself:
        - Suppose you want to estimate the proportion of Duke students who smoke. You collect a random sample of 100 students, where only 8 of them smoke. Can you use theoretical methods (Z) to construct a confidence interval based on these data? If not, describe how you could calculate a 95\% bootstrap confidence interval.

# Topic Break

Inference for numerical data:
    Topic Outcome:
        - Use the $t$-distribution for inference on a single mean, difference of paired (dependent) means, and difference of independent means.
        - Explain why the $t$-distribution helps make up for the additional variability introduced by using $s$ (sample standard deviation) in calculation of the standard error, in place of $\sigma$ (population standard deviation).
        - Describe how the $t$-distribution is different from the normal distribution, and what ?heavy tail? means in this context.
        - Note that the $t$-distribution has a single parameter, degrees of freedom, and as the degrees of freedom increases this distribution approaches the normal distribution.
        - Use a $t$-statistic, with degrees of freedom $df = n - 1$ for inference for a population mean:
        - Describe how to obtain a p-value for a $t$-test and a critical $t$-score ($t^\star_{df}$) for a confidence interval.
        - Reading: Section 7.1 of OpenIntro Statistics
        - Test yourself:
        - What is the $t^\star$ for a 95\% confidence interval for a mean, where the sample size is 13.
        - What is the p-value for a hypothesis test where the alternative hypothesis is two-sided, the sample size is 20, and the test statistic, T, is calculated to be 1.75?
        - Define observations as paired if each observation in one dataset has a special correspondence or connection with exactly one observation in the other data set.
        - Carry out inference for paired data by first subtracting the paired observations from each other, and then treating the set of differences as a new numerical variable on which to do inference (such as a confidence interval or hypothesis test for the average difference).
        - Calculate the standard error of the difference between means of two paired (dependent) samples as $SE = \frac{s_{diff}}{\sqrt{n_{diff}}}$ and use this standard error in hypothesis testing and confidence intervals comparing means of paired (dependent) groups.
        - Use a $t$-statistic, with degrees of freedom $df = n_{diff} - 1$ for inference for a population mean:
        - Recognize that a good interpretation of a confidence interval for the difference between two parameters includes a comparative statement (mentioning which group has the larger parameter).
        - Recognize that a confidence interval for the difference between two parameters that doesn?t include 0 is in agreement with a hypothesis test where the null hypothesis that sets the two parameters equal to each other is rejected.
        - Reading: Section 7.2 of OpenIntro Statistics
        - Test yourself:
        - 20 cardiac patients' blood pressure is measured before taking a medication, and after. For a given patient, are the before and after blood pressure measurements dependent (paired) or independent?
        - A random sample of 100 students were obtained and then randomly assigned into two equal sized groups. One group went on a roller coaster while the other in a simulator at an amusement park. Afterwards their blood pressure measurements were taken. Are the measurements dependent (paired) or independent?
        - Calculate the standard error of the difference between means of two independent samples as $SE = \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}$, and use this standard error in hypothesis testing and confidence intervals comparing means of independent groups.
        - Use a $t$-statistic, with degrees of freedom $df = min(n_1 - 1, n_2 - 1)$ for inference for a population mean:
        - Reading: Section 7.3 of OpenIntro Statistics
        - Test yourself:
        - Describe how the two sample means test is different from the paired means test, both conceptually and in terms of the calculation of the standard error.
        - A 95\% confidence interval for the difference between the number of calories consumed by mature and juvenile cats ($\mu_{mat} - \mu_{juv}$) is (80 calories, 100 calories). Interpret this interval, and determine if it suggests a significant difference between the two means.
        - Calculate the power of a test for a given effect size and significance level in two steps: (1) Find the cutoff for the sample statistic that will allow the null hypothesis to be rejected at the given significance level, (2) Calculate the probability of obtaining that sample statistic given the effect size.
        - Explain how power changes for changes in effect size, sample size, significance level, and standard error.
        - Define analysis of variance (ANOVA) as a statistical inference method that is used to determine if the variability in the sample means is so large that it seems unlikely to be from chance alone by simultaneously considering many groups at once.
        - Recognize that the null hypothesis in ANOVA sets all means equal to each other, and the alternative hypothesis suggest that at least one mean is different.
        - [] $H_0: \mu_1 = \mu_2 = \cdots = \mu_k$
        - [] $H_A:$ At least one mean is different
        - List the conditions necessary for performing ANOVA
        - [(1)] the observations should be independent within and across groups
        - [(2)] the data within each group are nearly normal
        - [(3)] the variability across the groups is about equal
        - Recognize that the test statistic for ANOVA, the F statistic, is calculated as the ratio of the mean square between groups (MSG, variability between groups) and mean square error (MSE, variability within errors), and has two degrees of freedom, one for the numerator ($df_{G} = k - 1$, where $k$ is the number of groups) and one for the denominator ($df_{E} = n - k$, where $n$ is the total sample size).
        - Describe why calculation of the p-value for ANOVA is always ``one sided".
        - Describe why conducting many $t$-tests for differences between each pair of means leads to an increased Type 1 Error rate, and we use a corrected significance level (Bonferroni corection, $\alpha^\star = \alpha / K$, where $K$ is the e number of comparisons being considered) to combat inflating this error rate.
        - Describe why it is possible to reject the null hypothesis in ANOVA but not find significant differences between groups as a result of pairwise comparisons.
        - Reading: Section 7.4 and 7.5 of OpenIntro Statistics
        - Test yourself:
        - We would like to compare the average income of Americans who live in the Northeast, Midwest, South, and West. What are the appropriate hypotheses?
        - Suppose the sample in the question above has 1000 observations, what are the degrees of freedom associated with the F-statistic?
        - Suppose the null hypothesis is rejected. Describe how we would discover which regions' averages are different from each other? Make sure to discuss how many pairwise comparisons we would need to make, and what the corrected significance level would be.
        - What visualizations are useful for checking each of the conditions required for performing ANOVA?

# Topic Break

Introduction to linear regression:
    Topic Outcome:
        - Define the explanatory variable as the independent variable (predictor), and the response variable as the dependent variable (predicted).
        - Plot the explanatory variable ($x$) on the x-axis and the response variable ($y$) on the y-axis, and fit a linear regression model
        - When describing the association between two numerical variables, evaluate
        - Define correlation as the \emph{linear} association between two numerical variables.
        - Note that correlation coefficient ($r$, also called Pearson's $r$) the following properties:
        - Recall that correlation does not imply causation.
        - Define residual ($e$) as the difference between the observed ($y$) and predicted ($\hat{y}$) values of the response variable.
        - Reading: Section 8.1 of OpenIntro Statistics
        - Test yourself:
        - Someone hands you the scatter diagram shown below, but has forgotten to label the axes. Can you calculate the correlation coefficient? Or do you need the labels?
        - A teaching assistant gives a quiz. There are 10 questions on the quiz and no partial credit is given. After grading the papers the TA writes down for each student the number of questions the student got right and the number wrong. What is the correlation of the number of questions right and wrong? \\
        - Suppose you fit a linear regression model predicting score on an exam from number of hours studied. Say you've studied for 4 hours. Would you prefer to be on the line, below the line, or above the line? What would the residual for your score be (0, negative, or positive)?
        - Define the least squares line as the line that minimizes the sum of the squared residuals, and list conditions necessary for fitting such line:
        - [(1)] linearity
        - [(2)] nearly normal residuals
        - [(3)] constant variability
        - Define an indicator variable as a binary explanatory variable (with two levels).
        - Calculate the estimate for the slope ($b_1$) as
        - Interpret the slope as
        - Note that the least squares line always passes through the average of the response and explanatory variables ($\bar{x},\bar{y}$).
        - Use the above property to calculate the estimate for the slope ($b_0$) as
        - Interpret the intercept as
        - Predict the value of the response variable for a given value of the explanatory variable, $x^\star$, by plugging in $x^\star$ in the in the linear model:
        - Define $R^2$ as the percentage of the variability in the response variable explained by the the explanatory variable.
        - Reading: Section 8.2 of OpenIntro Statistics
        - Test yourself:
        - We would not want to fit a least squares line to the data shown in the scatterplot below. Which of the conditions does it appear to violate?
        - Derive the formula for $b_0$ given the fact that the linear model is $\hat{y} = b_0 + b_1 \times x$ and that the least squares line goes through ($\bar{x}, \bar{y}$).
        - One study on male college students found their average height to be 70 inches with a standard deviation of 2 inches. Their average weight was 140 pounds, with a standard deviation of 25 pounds. The correlation between their height and weight was 0.60. Assuming that the two variables are linearly associated, write the linear model for predicting weight from height.
        - Is a male who is 72 inches tall and who weighs 115 pounds on the line, below the line, or above the line?
        - Describe what is an indicator variable, and what levels 0 and 1 mean for such variables.
        - The model below predicts GPA based on an indicator variable (0: not premed, 1: premed). Interpret the intercept and slope estimates in context of the data.
        - If the correlation between two variables $y$ and $x$ is 0.6, what percent of the variability in $y$ does $x$ explain?
        - Define a leverage point as a point that lies away from the center of the data in the horizontal direction.
        - Define an influential point as a point that influences (changes) the slope of the regression line.
        - Do not remove outliers from an analysis without good reason.
        - Be cautious about using a categorical explanatory variable when one of the levels has very few observations, as these may act as influential points.
        - Reading: Section 8.3 of OpenIntro Statistics
        - Test yourself: Determine if each of the three unusual observations in the plot below would be considered just an outlier, a leverage point, or an influential point.
        - Determine whether an explanatory variable is a significant predictor for the response variable using the $t$-test and the associated p-value in the regression output.
        - Set the null hypothesis testing for the significance of the predictor as $H_0: \beta_1 = 0$, and recognize that the standard software output yields the p-value for the two-sided alternative hypothesis.
        - Calculate the T score for the hypothesis test as
        - Note that a hypothesis test for the intercept is often irrelevant since it's usually out of the range of the data, and hence it is usually an extrapolation.
        - Calculate a confidence interval for the slope as
        - Reading: Section 8.4 of OpenIntro Statistics
        - Test yourself:
        - Given the regression output below for predicting $y$ from $x$ where $n = 100$, confirm the T score and the p-value, determine whether $x$ is a significant predictor of $y$, and interpret the p-value in context.
        - Calculate a 95\% confidence interval for the slope given above.

# Topic Break

Multiple and logistic regression:
    Topic Outcome:
        - Define the multiple linear regression model as
        - Interpret the estimate for the intercept ($b_0$) as the expected value of $y$ when all predictors are equal to 0, on average.
        - Interpret the estimate for a slope (say $b_1$) as ``All else held constant, for each unit increase in $x_1$, we would expect $y$ to increase/decrease on average by $b_1$."
        - Define collinearity as a high correlation between two independent variables such that the two variables contribute redundant information to the model -- which is something we want to avoid in multiple linear regression.
        - Note that $R^2$ will increase with each explanatory variable added to the model, regardless of whether or not the added variables is a meaningful predictor of the response variable. Therefore we use adjusted $R^2$, which applies a penalty for the number of predictors included in the model, to better assess the strength of a multiple linear regression model:
        - Reading: Section 9.1 of OpenIntro Statistics
        - Test yourself:
        - How is multiple linear regression different than simple linear regression?
        - What does ``all else held constant" mean in the interpretation of a slope coefficient in multiple linear regression?
        - What is collinearity? Why do we want to avoid collinearity in multiple regression models?
        - Explain the difference between $R^2$ and adjusted $R^2$. Which one will be higher? Which one tells us the variability in $y$ explained by the model? Which one is a better measure of the strength of a linear regression model? Why?
        - Define model selection as identifying the best model for predicting a given response variable.
        - Note that we usually prefer simpler (parsimonious) models over more complicated ones.
        - Define the full model as the model with all explanatory variables included as predictors.
        - Note that the p-values associated with each predictor are conditional on other variables being included in the model, so they can be used to assess if a given predictor is significant, given that all others are in the model.
        - Stepwise model selection (backward or forward) can be done based based on adjusted $R^2$ (choose the model with higher adjusted $R^2$).
        - The general idea behind backward-selection is to start with the full model and eliminate one variable at a time until the ideal model is reached.
        - [(i)] Start with the full model.
        - [(ii)] Refit all possible models omitting one variable at a time, and choose the model with the highest adjusted $R^2$.
        - [(iii)] Repeat until maximum possible adjusted $R^2$ is reached.
        - The general idea behind forward-selection is to start with only one variable and adding one variable at a time until the ideal model is reached.
        - [(i)] Try all possible simple linear regression models predicting $y$ using one explanatory variable at a time. Choose the model with the highest adjusted $R^2$.
        - [(ii)] Try all possible models adding one more explanatory variable at a time, and choose the model with the highest adjusted $R^2$.
        - [(iii)] Repeat until maximum possible adjusted $R^2$ is reached.
        - Adjusted $R^2$ method is more computationally intensive, but it is more reliable, since it doesn't depend on an arbitrary significant level.
        - Reading: Section 9.2 of OpenIntro Statistics
        - Test yourself:
        - Define the term ``parsimonious model".
        - Describe the backward-selection algorithm using adjusted $R^2$ as the criterion for model selection.
        - List the conditions for multiple linear regression as
        - [(1)] linear relationship between each (numerical) explanatory variable and the response - checked using scatterplots of $y$ vs. each $x$, and residuals plots of $residuals$ vs. each $x$
        - [(2)] nearly normal residuals with mean 0 - checked using a normal probability plot and histogram of residuals
        - [(3)] constant variability of residuals - checked using residuals plots of $residuals$ vs. $\hat{y}$, and $residuals$ vs. each $x$
        - [(4)] independence of residuals (and hence observations) - checked using a scatterplot of $residuals$ vs. order of data collection (will reveal non-independence if data have time series structure)
        - Note that no model is perfect, but even imperfect models can be useful.
        - Reading: Section 9.3 and 9.4 of OpenIntro Statistics
        - Test yourself:
        - If a residuals plot ($residuals$ vs. $x$ or $residuals$ vs. $\hat{y}$) shows a fan shape, we worry about non-constant variability of residuals. What would the shape of these residuals look like if absolute value of residuals are plotted against a predictor or $\hat{y}$.

